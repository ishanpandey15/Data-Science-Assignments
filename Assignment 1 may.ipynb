{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b53a0a68-250a-4bf9-9cef-7f2decbf5455",
   "metadata": {},
   "source": [
    "Q1. What is a contingency matrix, and how is it used to evaluate the performance of a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7500b696-5a5d-4fa5-a4ce-af1f2a09e73c",
   "metadata": {},
   "source": [
    "A contingency matrix, also known as a confusion matrix or classification table, is a table used to evaluate the performance of a classification model, typically in the context of binary classification tasks (although it can be extended to multi-class problems as well). It summarizes the model's predictions and the actual class labels of a dataset. The matrix is organized into four categories, which help in assessing various aspects of classification performance:\n",
    "\n",
    "Let's define the four categories in a binary classification context:\n",
    "\n",
    "True Positives (TP):\n",
    "\n",
    "These are instances that were correctly classified as positive (class 1) by the model.\n",
    "True Negatives (TN):\n",
    "\n",
    "These are instances that were correctly classified as negative (class 0) by the model.\n",
    "False Positives (FP):\n",
    "\n",
    "These are instances that were incorrectly classified as positive by the model when they actually belong to the negative class. Also known as Type I errors.\n",
    "False Negatives (FN):\n",
    "\n",
    "These are instances that were incorrectly classified as negative by the model when they actually belong to the positive class. Also known as Type II errors.\n",
    "The contingency matrix is organized as follows:\n",
    "\n",
    "sql\n",
    "Copy code\n",
    "            Actual\n",
    "           +-------+-------+\n",
    "           | True  | False |\n",
    "+----------+-------+-------+\n",
    "| Predict  |       |       |\n",
    "| +-------+-------+-------+\n",
    "| | True  |  TP   |  FP   |\n",
    "| +-------+-------+-------+\n",
    "| | False |  FN   |  TN   |\n",
    "+----------+-------+-------+\n",
    "The entries in the matrix provide the counts of instances in each of these categories. Once you have the contingency matrix, you can compute various performance metrics to evaluate the classification model's effectiveness, such as:\n",
    "\n",
    "Accuracy: The proportion of correctly classified instances (TP and TN) out of the total instances.\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "Precision (Positive Predictive Value): The proportion of true positives among all instances predicted as positive. It measures the model's ability to avoid false positives.\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "Recall (Sensitivity, True Positive Rate): The proportion of true positives among all actual positive instances. It measures the model's ability to capture positive instances.\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "Specificity (True Negative Rate): The proportion of true negatives among all actual negative instances.\n",
    "\n",
    "Specificity = TN / (TN + FP)\n",
    "F1-Score: The harmonic mean of precision and recall, which balances the trade-off between precision and recall.\n",
    "\n",
    "F1-Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "False Positive Rate (FPR): The proportion of false positives among all actual negative instances.\n",
    "\n",
    "FPR = FP / (TN + FP)\n",
    "False Negative Rate (FNR): The proportion of false negatives among all actual positive instances.\n",
    "\n",
    "FNR = FN / (TP + FN)\n",
    "The choice of which metrics to emphasize depends on the specific goals of the classification task. For example, in a medical diagnosis task, recall (sensitivity) might be more important to ensure that all positive cases are correctly identified, even if it means accepting some false positives (lower precision). In contrast, in spam email detection, precision may be prioritized to minimize false alarms, even at the cost of missing some spam emails (lower recall)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f51dfd5-a4fc-4b35-ba97-2f02a6b5b7bf",
   "metadata": {},
   "source": [
    "Q2. How is a pair confusion matrix different from a regular confusion matrix, and why might it be useful in\n",
    "certain situations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a9b909-0ef5-44ba-b8a2-986cc9441df3",
   "metadata": {},
   "source": [
    "A pair confusion matrix, also known as a pairwise confusion matrix or a confusion matrix for pairwise classification, is a variation of the traditional confusion matrix used in multi-class classification problems, particularly when dealing with imbalanced datasets or situations where you want to focus on pairwise classification performance. It is designed to assess the binary classification performance between each pair of classes in a multi-class problem.\n",
    "\n",
    "Here's how a pair confusion matrix differs from a regular confusion matrix:\n",
    "\n",
    "Regular Confusion Matrix (Multi-Class):\n",
    "\n",
    "In a regular confusion matrix, each row represents the actual class, and each column represents the predicted class.\n",
    "It is used to evaluate the performance of a multi-class classification model by providing counts of true positives, true negatives, false positives, and false negatives for each class.\n",
    "Pair Confusion Matrix (Pairwise Classification):\n",
    "\n",
    "In a pair confusion matrix, it is constructed for each pair of classes (one vs. one classification).\n",
    "Each pair confusion matrix focuses on the binary classification problem of distinguishing one specific class from another (e.g., Class A vs. Class B).\n",
    "It is particularly useful when dealing with multi-class classification problems with imbalanced classes or when you want to assess the performance of the classifier for specific class pairs.\n",
    "The structure of a pair confusion matrix is similar to that of a regular binary confusion matrix:\n",
    "\n",
    "lua\n",
    "Copy code\n",
    "          Actual\n",
    "         +-------+-------+\n",
    "         | Class | Class |\n",
    "+--------+-------+-------+\n",
    "| Predict |       |       |\n",
    "| +-------+-------+-------+\n",
    "| | Class |  TP   |  FP   |\n",
    "| +-------+-------+-------+\n",
    "| | Class |  FN   |  TN   |\n",
    "+--------+-------+-------+\n",
    "Usefulness of Pair Confusion Matrix:\n",
    "\n",
    "Pair confusion matrices can be useful in several situations:\n",
    "\n",
    "Imbalanced Datasets: When you have imbalanced classes in a multi-class problem, assessing the performance of individual class pairs can provide more insight into how well the classifier is performing for specific challenging class combinations.\n",
    "\n",
    "Specific Pairwise Evaluation: In some applications, you may be more interested in the performance of the classifier for specific class pairs (e.g., distinguishing between critical classes). Pairwise evaluation allows you to focus on those specific comparisons.\n",
    "\n",
    "Class-Dependent Performance Analysis: In cases where different classes have significantly different importance or cost associated with misclassification, pair confusion matrices allow you to tailor the evaluation to the specific needs of each class pair.\n",
    "\n",
    "Model Comparison: When comparing the performance of different classifiers or models on a multi-class problem, using pair confusion matrices can provide a more detailed and nuanced view of their relative strengths and weaknesses for different class pairs.\n",
    "\n",
    "However, it's important to note that using pair confusion matrices increases the number of binary classifications performed, which can be computationally more expensive and may require careful consideration when interpreting the results, especially if you need to aggregate or summarize performance across multiple class pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9552b9e-d73d-4bce-88b8-c44ff1d06364",
   "metadata": {},
   "source": [
    "Q3. What is an extrinsic measure in the context of natural language processing, and how is it typically\n",
    "used to evaluate the performance of language models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1dfda4-5824-40e6-a234-6ebfa0dbb974",
   "metadata": {},
   "source": [
    "In the context of natural language processing (NLP), extrinsic measures are evaluation metrics or methods that assess the performance of language models, algorithms, or systems based on their ability to solve specific real-world tasks or applications. Extrinsic measures evaluate how well a language model or NLP system performs in the context of a broader application, rather than focusing solely on its internal language processing capabilities.\n",
    "\n",
    "Here's how extrinsic measures are typically used to evaluate the performance of language models:\n",
    "\n",
    "Task-Specific Evaluation: Extrinsic measures are designed to evaluate language models within the context of specific NLP tasks or applications, such as machine translation, text classification, sentiment analysis, speech recognition, and information retrieval. These tasks require the model to process and understand language to achieve a particular goal.\n",
    "\n",
    "Real-World Performance: Extrinsic measures assess how well a language model or system performs in real-world scenarios and how effectively it contributes to solving practical problems. This evaluation goes beyond assessing the model's linguistic capabilities and considers its utility in applications.\n",
    "\n",
    "End-to-End Evaluation: Extrinsic measures often involve end-to-end evaluation, meaning that the entire system or pipeline is assessed, including components such as pre-processing, feature extraction, model training, and post-processing. This holistic evaluation accounts for the impact of all components on task performance.\n",
    "\n",
    "Metrics and Benchmarks: Specific evaluation metrics and benchmarks are defined for each task or application. These metrics could be accuracy, precision, recall, F1-score, BLEU score (for machine translation), perplexity (for language modeling), or any other appropriate measure for the specific task.\n",
    "\n",
    "Human Evaluation: In some cases, human annotators or judges may be involved in extrinsic evaluation to assess the quality of the model's output, especially for tasks involving natural language generation or understanding. Human judgments can provide valuable insights into the model's performance.\n",
    "\n",
    "Comparative Analysis: Extrinsic measures allow for comparative analysis between different language models or NLP systems. Researchers and practitioners can use these measures to determine which model or approach is the most effective for a given task.\n",
    "\n",
    "Fine-Tuning and Optimization: Extrinsic evaluation helps guide the fine-tuning and optimization of language models for specific tasks. Researchers can use feedback from extrinsic evaluations to make improvements and adapt models to better suit the target application.\n",
    "\n",
    "Examples of extrinsic evaluation tasks in NLP include sentiment analysis accuracy, machine translation BLEU score, named entity recognition F1-score, question-answering accuracy, and speech recognition word error rate. These measures provide a more practical and meaningful assessment of how language models and NLP systems perform in real-world applications, which is crucial for their development and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4c6175-7018-4954-a2a9-262247b07b9f",
   "metadata": {},
   "source": [
    "Q4. What is an intrinsic measure in the context of machine learning, and how does it differ from an\n",
    "extrinsic measure?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901c56da-9d1b-464d-8b03-3f71d7ba2c79",
   "metadata": {},
   "source": [
    "In the context of machine learning and natural language processing (NLP), intrinsic measures and extrinsic measures are two types of evaluation methods used to assess the performance of models, algorithms, or systems. They differ in terms of what aspects of performance they assess and the level of abstraction at which they operate.\n",
    "\n",
    "Intrinsic Measures:\n",
    "\n",
    "Internal Evaluation: Intrinsic measures focus on evaluating the performance of a model or algorithm based on its internal characteristics or capabilities. These characteristics are typically related to the model's inherent abilities to process and understand data.\n",
    "\n",
    "Isolated Evaluation: Intrinsic measures assess a model in isolation, without considering its performance in a broader application context. They are concerned with how well the model performs specific sub-tasks or components of a larger system.\n",
    "\n",
    "Example Intrinsic Measures:\n",
    "\n",
    "In language modeling, perplexity is an intrinsic measure that assesses how well a language model predicts a sequence of words based on its learned language probabilities. It quantifies the model's ability to generate text that resembles a given dataset.\n",
    "In image classification, accuracy or cross-entropy loss can be considered intrinsic measures because they evaluate how well a neural network classifies images based on its learned weights.\n",
    "Extrinsic Measures:\n",
    "\n",
    "Task-Specific Evaluation: Extrinsic measures evaluate the performance of a model or system within the context of a specific real-world task or application. They assess how effectively the model contributes to solving practical problems.\n",
    "\n",
    "Real-World Performance: Extrinsic measures assess a model's performance in real-world scenarios and consider its utility in addressing broader applications. They evaluate the system as a whole, including all components and processes.\n",
    "\n",
    "Example Extrinsic Measures:\n",
    "\n",
    "In machine translation, BLEU score is an extrinsic measure that assesses the quality of machine-generated translations based on human reference translations. It evaluates how well the translation system performs the actual translation task.\n",
    "In sentiment analysis, accuracy or F1-score can be considered extrinsic measures because they assess how well a sentiment analysis model classifies text for sentiment-related tasks, which are real-world applications.\n",
    "Key Differences:\n",
    "\n",
    "Focus: Intrinsic measures assess internal model capabilities, while extrinsic measures assess real-world task performance.\n",
    "Isolation vs. Integration: Intrinsic measures evaluate models in isolation, while extrinsic measures consider models as part of a larger system or application.\n",
    "Scope: Intrinsic measures focus on specific sub-tasks or components, while extrinsic measures assess overall task or application performance.\n",
    "Metrics: Intrinsic measures often use domain-specific metrics (e.g., perplexity for language models), while extrinsic measures typically use task-specific metrics (e.g., BLEU score for machine translation).\n",
    "In practice, both intrinsic and extrinsic measures are valuable for evaluating machine learning models and NLP systems. Intrinsic measures help assess model capabilities and fine-tune algorithms, while extrinsic measures provide insights into how well models perform in real-world applications. Researchers and practitioners often use a combination of both types of evaluation to gain a comprehensive understanding of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc97a51-ca31-49c8-a482-0433443bc294",
   "metadata": {},
   "source": [
    "Q5. What is the purpose of a confusion matrix in machine learning, and how can it be used to identify\n",
    "strengths and weaknesses of a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c220b76d-6c73-4f85-99e1-2e1390296d7b",
   "metadata": {},
   "source": [
    "A confusion matrix is a fundamental tool in machine learning for evaluating the performance of classification models, such as those used in binary or multi-class classification tasks. Its primary purpose is to provide a detailed breakdown of the model's predictions and actual class labels, which can help in assessing the strengths and weaknesses of the model's performance.\n",
    "\n",
    "Here's how a confusion matrix is structured and how it can be used:\n",
    "\n",
    "Structure of a Confusion Matrix:\n",
    "A confusion matrix is organized into four categories based on the model's predictions and the actual class labels:\n",
    "\n",
    "True Positives (TP): Instances correctly classified as positive by the model.\n",
    "True Negatives (TN): Instances correctly classified as negative by the model.\n",
    "False Positives (FP): Instances incorrectly classified as positive by the model (Type I errors).\n",
    "False Negatives (FN): Instances incorrectly classified as negative by the model (Type II errors).\n",
    "The matrix layout typically looks like this:\n",
    "\n",
    "sql\n",
    "Copy code\n",
    "            Actual\n",
    "           +-------+-------+\n",
    "           | True  | False |\n",
    "+----------+-------+-------+\n",
    "| Predict  |       |       |\n",
    "| +-------+-------+-------+\n",
    "| | True  |  TP   |  FP   |\n",
    "| +-------+-------+-------+\n",
    "| | False |  FN   |  TN   |\n",
    "+----------+-------+-------+\n",
    "Using a Confusion Matrix to Identify Strengths and Weaknesses:\n",
    "\n",
    "Accuracy Assessment: You can calculate basic classification metrics directly from the confusion matrix, such as accuracy, which measures the proportion of correctly classified instances:\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "High accuracy indicates overall good performance, but it doesn't tell the full story.\n",
    "\n",
    "Precision and Recall Analysis: Precision (positive predictive value) and recall (true positive rate) provide insights into the trade-off between false positives and false negatives. High precision indicates that when the model predicts a positive class, it's usually correct. High recall indicates that the model can capture most positive instances.\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "Recall = TP / (TP + FN)\n",
    "F1-Score: The F1-score is the harmonic mean of precision and recall, offering a balanced view of a model's performance, especially when classes are imbalanced. It considers both false positives and false negatives.\n",
    "\n",
    "F1-Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "Specificity and False Positive Rate: These metrics are useful when the cost of false positives is a concern. High specificity indicates that the model correctly identifies the negative class, and a low false positive rate indicates that it doesn't produce many false alarms.\n",
    "\n",
    "Specificity = TN / (TN + FP)\n",
    "False Positive Rate = FP / (TN + FP)\n",
    "Understanding Class Imbalances: The confusion matrix helps identify class imbalances, where one class significantly outnumbers the other. It can highlight cases where the model struggles with the minority class.\n",
    "\n",
    "Threshold Adjustment: By analyzing the confusion matrix, you can decide whether to adjust the classification threshold. Depending on the application, you may prioritize precision, recall, or another metric.\n",
    "\n",
    "Diagnostic Insights: The confusion matrix can provide diagnostic insights into the types of errors the model makes. For example, if false negatives are common, it may indicate that the model has difficulty identifying positive instances.\n",
    "\n",
    "In summary, a confusion matrix serves as a foundational tool for assessing the performance of classification models. It allows you to go beyond simple accuracy and gain a deeper understanding of how the model behaves with respect to different classes, providing valuable insights for model refinement and improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a158be5-94d2-4684-a6b4-e2d75bda2bc2",
   "metadata": {},
   "source": [
    "Q6. What are some common intrinsic measures used to evaluate the performance of unsupervised\n",
    "learning algorithms, and how can they be interpreted?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e833efca-c782-47e6-a439-3a4adc7dcb10",
   "metadata": {},
   "source": [
    "Unsupervised learning algorithms are used to discover patterns, structure, or relationships in data without the guidance of labeled target variables. Unlike supervised learning, where you have explicit labels for evaluation, the evaluation of unsupervised learning algorithms often relies on intrinsic measures that assess the quality of the learned representations, clusters, or structure within the data. Here are some common intrinsic measures used to evaluate unsupervised learning algorithms and how they can be interpreted:\n",
    "\n",
    "Silhouette Score:\n",
    "\n",
    "The silhouette score measures the quality of clusters in a clustering task. It quantifies how similar each data point is to its own cluster (cohesion) compared to other clusters (separation).\n",
    "Range: -1 (poor clustering) to 1 (perfect clustering).\n",
    "Interpretation:\n",
    "High Silhouette Score (close to 1): Indicates that data points within the same cluster are well grouped and well separated from other clusters.\n",
    "Low Silhouette Score (close to -1): Suggests overlapping or misclassified clusters.\n",
    "Davies-Bouldin Index:\n",
    "\n",
    "The Davies-Bouldin Index measures the average similarity between each cluster and its most similar cluster. Lower values indicate better clustering.\n",
    "Interpretation:\n",
    "Lower Davies-Bouldin Index: Indicates well-separated and distinct clusters.\n",
    "Higher Davies-Bouldin Index: Suggests clusters that are not well-separated.\n",
    "Dunn Index:\n",
    "\n",
    "The Dunn Index quantifies the ratio of the minimum inter-cluster distance to the maximum intra-cluster distance. Higher values are desirable.\n",
    "Interpretation:\n",
    "Higher Dunn Index: Suggests well-separated clusters with compact intra-cluster data points.\n",
    "Lower Dunn Index: Indicates that clusters are not well-separated or that data points within clusters are widely dispersed.\n",
    "Inertia (Within-Cluster Sum of Squares):\n",
    "\n",
    "Inertia measures the sum of squared distances of data points to their cluster center (centroid) in K-means clustering.\n",
    "Interpretation:\n",
    "Lower Inertia: Indicates that data points are tightly clustered around their centroids, suggesting well-defined clusters.\n",
    "Higher Inertia: Suggests more dispersed data points within clusters.\n",
    "Calinski-Harabasz Index (Variance Ratio Criterion):\n",
    "\n",
    "The Calinski-Harabasz Index is based on the ratio of between-cluster variance to within-cluster variance. Higher values indicate better clustering.\n",
    "Interpretation:\n",
    "Higher Calinski-Harabasz Index: Suggests well-separated clusters.\n",
    "Lower Calinski-Harabasz Index: Indicates that clusters may overlap or are not well-defined.\n",
    "Dendrogram Visualization:\n",
    "\n",
    "Dendrograms are hierarchical cluster tree structures often used to visualize the results of hierarchical clustering algorithms. They provide insights into the hierarchy and structure of clusters.\n",
    "Interpretation:\n",
    "Branches close to the root of the dendrogram represent broader clusters, while branches closer to the leaves represent finer, more detailed clusters.\n",
    "Dimension Reduction Quality:\n",
    "\n",
    "In dimensionality reduction techniques such as Principal Component Analysis (PCA) or t-Distributed Stochastic Neighbor Embedding (t-SNE), the quality of dimensionality reduction can be assessed by how well the reduced-dimensional data preserves the original data's structure and relationships. Visual inspection and intrinsic measures like explained variance can be used.\n",
    "Interpreting these intrinsic measures often involves comparing them across different parameter settings or algorithms to select the most suitable model or parameter configuration for the specific unsupervised learning task. Keep in mind that the choice of measure may depend on the nature of the data and the goals of the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44257e86-df98-4749-9f74-05d97607ed65",
   "metadata": {},
   "source": [
    "Q7. What are some limitations of using accuracy as a sole evaluation metric for classification tasks, and\n",
    "how can these limitations be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eb1663-152c-41de-a9b9-e5674c70cb85",
   "metadata": {},
   "source": [
    "Accuracy is a commonly used evaluation metric for classification tasks, but it has limitations that can affect its suitability for assessing model performance, especially in situations where class distribution is imbalanced or when different types of classification errors have varying consequences. Here are some limitations of using accuracy as a sole evaluation metric and ways to address them:\n",
    "\n",
    "Imbalanced Classes:\n",
    "\n",
    "Limitation: In imbalanced datasets where one class dominates, a model can achieve high accuracy by simply predicting the majority class. This can lead to a misleading assessment of model performance.\n",
    "Solution: Consider using additional evaluation metrics, such as precision, recall, F1-score, or the area under the Receiver Operating Characteristic curve (AUC-ROC), which provide a more balanced view of model performance.\n",
    "Different Costs of Errors:\n",
    "\n",
    "Limitation: Accuracy treats all types of classification errors (false positives and false negatives) equally, even though their consequences may differ significantly in real-world applications.\n",
    "Solution: Depending on the specific problem, assign different costs or weights to different types of errors and use metrics like weighted accuracy, cost-sensitive classification, or custom loss functions that reflect the problem's requirements.\n",
    "Misleading in Rare Events:\n",
    "\n",
    "Limitation: In tasks involving rare events or anomalies, high accuracy may be achieved by correctly classifying the majority class while failing to identify the rare class.\n",
    "Solution: Use metrics such as precision, recall, or F1-score that focus on the performance of the minority or rare class to assess the model's ability to detect these important instances.\n",
    "Threshold Dependency:\n",
    "\n",
    "Limitation: Accuracy is threshold-dependent, meaning that changing the classification threshold can significantly affect accuracy. The choice of threshold may be arbitrary.\n",
    "Solution: Evaluate models across different threshold values and use metrics like the Receiver Operating Characteristic curve (ROC curve) or precision-recall curve to make informed decisions about threshold selection.\n",
    "Ignoring Class Probabilities:\n",
    "\n",
    "Limitation: Accuracy only considers the final class predictions and ignores the probabilistic nature of many classification algorithms. It doesn't provide information about the model's confidence or uncertainty.\n",
    "Solution: Examine class probabilities, confidence intervals, or model calibration curves to gain insights into the model's level of certainty about its predictions.\n",
    "Label Noise and Errors:\n",
    "\n",
    "Limitation: Accuracy assumes that ground truth labels are completely accurate, which may not be the case in real-world datasets.\n",
    "Solution: Perform thorough data cleaning and validation to address label noise issues. Additionally, consider using robust evaluation techniques like cross-validation and bootstrapping to assess model stability.\n",
    "Multiclass Classification:\n",
    "\n",
    "Limitation: In multiclass classification, accuracy can be misleading because it treats all classes equally. Imbalanced class distributions among multiple classes can skew the results.\n",
    "Solution: Use metrics designed for multiclass problems, such as macro-averaged or micro-averaged precision, recall, and F1-score, which provide a more comprehensive view of class-specific performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1119d74d-f60f-4273-924e-0ed412a7dd78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
