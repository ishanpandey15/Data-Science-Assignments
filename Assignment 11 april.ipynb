{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b55435-9b67-4f96-b42e-d4f961c7b78e",
   "metadata": {},
   "source": [
    "Q1. What is an ensemble technique in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55562593-fd70-4ba5-9874-9676f8f78c13",
   "metadata": {},
   "source": [
    "In machine learning, an ensemble technique is a method that combines the predictions or decisions of multiple individual models (often referred to as \"base models\" or \"base learners\") to create a stronger, more accurate, and more robust model. The fundamental idea behind ensemble techniques is that by aggregating the outputs of multiple models, the ensemble can often achieve better overall performance than any individual model in the ensemble. Ensembles are widely used in machine learning for both classification and regression tasks.\n",
    "\n",
    "The two main types of ensemble techniques are:\n",
    "\n",
    "Bagging (Bootstrap Aggregating):\n",
    "\n",
    "Bagging involves training multiple base models independently on different subsets of the training data. These subsets are typically created through a process called bootstrapping (sampling with replacement).\n",
    "Each base model provides its own prediction, and the ensemble combines these predictions, often by averaging (for regression) or majority voting (for classification).\n",
    "A well-known ensemble method that uses bagging is the Random Forest algorithm.\n",
    "Boosting:\n",
    "\n",
    "Boosting is an ensemble technique that combines multiple base models sequentially. Each base model is trained to correct the errors of the previous ones.\n",
    "Weak learners (models that perform slightly better than random guessing) are typically used as base models. Boosting algorithms assign different weights to training examples, emphasizing the importance of examples that were misclassified by earlier base models.\n",
    "The final prediction is a weighted combination of the predictions made by each base model.\n",
    "Popular boosting algorithms include AdaBoost, Gradient Boosting (e.g., XGBoost, LightGBM), and others.\n",
    "Ensemble techniques offer several advantages, including improved predictive accuracy, better generalization, and enhanced robustness to noise and outliers. They are widely used in machine learning competitions and real-world applications to achieve state-of-the-art results.\n",
    "\n",
    "Ensemble techniques can be applied to a wide range of machine learning models, including decision trees, linear models, neural networks, and more. The choice of ensemble method and the configuration of its parameters depend on the specific problem and dataset, and they often require careful tuning and experimentation to achieve optimal results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa44a0-5e1b-4977-91cc-439d4ec68f4a",
   "metadata": {},
   "source": [
    "Q2. Why are ensemble techniques used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbead0da-5498-44c6-878d-8aa9e634a6cf",
   "metadata": {},
   "source": [
    "Ensemble techniques are used in machine learning for several compelling reasons:\n",
    "\n",
    "Improved Predictive Performance:\n",
    "\n",
    "Ensemble methods often yield higher predictive accuracy compared to individual base models. By combining the predictions of multiple models, the ensemble can capture a broader range of patterns in the data, reducing both bias and variance.\n",
    "Better Generalization:\n",
    "\n",
    "Ensembles are effective at reducing overfitting, which occurs when a model performs well on the training data but poorly on unseen data. By combining multiple models, ensembles can provide better generalization to new, unseen data, improving model robustness.\n",
    "Robustness to Noise and Outliers:\n",
    "\n",
    "Ensembles are more robust to noisy data and outliers than individual models. Outliers can have a disproportionate impact on a single model, but their influence can be mitigated when combined with predictions from other models.\n",
    "Handling Complex Relationships:\n",
    "\n",
    "Ensemble methods can capture complex relationships in the data by leveraging diverse base models. This is particularly valuable when the underlying data has intricate, nonlinear patterns.\n",
    "Model Stability:\n",
    "\n",
    "Ensembles are often more stable and less sensitive to variations in the training data. Small changes in the training data or initial conditions are less likely to result in significant changes in ensemble predictions.\n",
    "Versatility:\n",
    "\n",
    "Ensemble techniques can be applied to various machine learning algorithms and models, making them versatile and applicable to a wide range of problems, from regression to classification.\n",
    "State-of-the-Art Performance:\n",
    "\n",
    "Ensembles are commonly used in machine learning competitions and real-world applications to achieve state-of-the-art results. They have been instrumental in pushing the boundaries of what is achievable in predictive modeling.\n",
    "Model Interpretability:\n",
    "\n",
    "In some cases, ensembles can provide insights into feature importance or model contributions, making it possible to understand which aspects of the data are most influential in making predictions.\n",
    "Reduced Model Bias:\n",
    "\n",
    "Ensembles can help reduce model bias by aggregating predictions from different perspectives. This can be particularly valuable when dealing with imbalanced datasets or biased training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07665797-f5c1-4e48-9be8-013d650b1bb8",
   "metadata": {},
   "source": [
    "Q3. What is bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbc5508-b56e-4cc9-924f-cadb83e1f5c5",
   "metadata": {},
   "source": [
    "Bagging, short for Bootstrap Aggregating, is an ensemble technique in machine learning that aims to improve the predictive performance and robustness of models by combining the predictions of multiple base models. The core idea behind bagging is to train multiple base models independently on different subsets of the training data and then combine their predictions.\n",
    "\n",
    "Here's how the bagging process works:\n",
    "\n",
    "Bootstrapping:\n",
    "\n",
    "Bagging begins by creating multiple subsets (samples) of the training data through a process called bootstrapping. Bootstrapping involves randomly sampling the training data with replacement. As a result, each subset may contain duplicate data points, and some data points may be excluded entirely.\n",
    "Base Model Training:\n",
    "\n",
    "Each of the bootstrapped subsets is used to train an individual base model. These base models can be any machine learning algorithm, such as decision trees, random forests, support vector machines, or neural networks.\n",
    "Importantly, the base models are trained independently of each other, meaning they don't share information during training.\n",
    "Prediction Aggregation:\n",
    "\n",
    "Once all the base models are trained, they are used to make predictions on the same test dataset.\n",
    "For regression problems, the predictions of the base models are typically averaged to obtain the final ensemble prediction.\n",
    "For classification problems, the ensemble prediction can be determined by majority voting (class with the most votes) among the base models' predictions.\n",
    "The key benefits of bagging include:\n",
    "\n",
    "Reduction of Variance: By training base models on different subsets of data, bagging helps reduce the variance of the ensemble's predictions. This, in turn, leads to better generalization and less susceptibility to overfitting.\n",
    "\n",
    "Improved Robustness: Bagging makes the ensemble more robust to noisy data and outliers. Since each base model is exposed to a different subset of data, the impact of individual noisy data points is diminished when aggregating predictions.\n",
    "\n",
    "Parallelism: Bagging is inherently parallelizable, as each base model can be trained independently. This makes it suitable for distributed computing environments and can lead to faster training times.\n",
    "\n",
    "Applicability to Various Models: Bagging is a model-agnostic technique, meaning it can be applied to different types of base models, making it versatile for different machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188f1f48-eb58-4f75-ae6a-fbf04349b4bb",
   "metadata": {},
   "source": [
    "Q4. What is boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046f4632-84ce-44b1-9761-9ad2e76cb6a8",
   "metadata": {},
   "source": [
    "Boosting is an ensemble machine learning technique that combines the predictions of multiple base models (often weak learners) to create a strong ensemble model. Unlike bagging, where base models are trained independently, boosting algorithms train base models sequentially in a way that focuses on correcting the errors of the previous models. The core idea of boosting is to give more weight to misclassified data points during training, thereby emphasizing the challenging examples.\n",
    "\n",
    "Here's how boosting typically works:\n",
    "\n",
    "Initialization:\n",
    "\n",
    "Boosting starts with an initial dataset where each data point is assigned an equal weight.\n",
    "A weak learner (a model that performs slightly better than random guessing) is selected as the first base model.\n",
    "Sequential Training:\n",
    "\n",
    "In each boosting round (iteration), a new base model is trained.\n",
    "The base model's objective is to minimize the weighted classification error on the training data.\n",
    "The weights of the training data points are adjusted based on the performance of the previous models. Data points that were misclassified or had higher errors in earlier rounds receive higher weights.\n",
    "Weighted Combination:\n",
    "\n",
    "The predictions made by each base model in the ensemble are combined to form the final prediction.\n",
    "The combination often involves weighted majority voting (for classification) or weighted averaging (for regression).\n",
    "Updating Weights:\n",
    "\n",
    "After each round, the weights of the data points are updated.\n",
    "Misclassified data points are assigned higher weights to make them more influential in the next round.\n",
    "Correctly classified data points receive lower weights.\n",
    "Repeat:\n",
    "\n",
    "Steps 2 to 4 are repeated for a predefined number of boosting rounds or until a stopping criterion is met.\n",
    "Each new base model added to the ensemble is designed to improve the overall performance of the model.\n",
    "Final Ensemble Model:\n",
    "\n",
    "The final ensemble model is composed of all the base models trained in the sequential process.\n",
    "Each base model's contribution is weighted based on its performance and importance.\n",
    "Popular boosting algorithms include:\n",
    "\n",
    "AdaBoost (Adaptive Boosting): AdaBoost assigns weights to data points and focuses on misclassified samples in each round. It adjusts the weights of weak learners to emphasize the importance of harder-to-classify examples.\n",
    "\n",
    "Gradient Boosting: Gradient boosting builds an ensemble by training base models to minimize a loss function, such as mean squared error for regression or deviance for classification. It uses gradient descent to optimize the ensemble's predictions.\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting): XGBoost is an optimized and efficient implementation of gradient boosting that has become popular in machine learning competitions due to its speed and performance.\n",
    "\n",
    "LightGBM: LightGBM is another gradient boosting library that uses a histogram-based approach for training, making it fast and memory-efficient.\n",
    "\n",
    "Boosting algorithms are widely used in machine learning for both classification and regression tasks and are known for their ability to achieve high accuracy and robustness on a variety of datasets. They are essential tools in the machine learning practitioner's toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a45291a-c165-416e-bf14-45abe95b56bd",
   "metadata": {},
   "source": [
    "Q5. What are the benefits of using ensemble techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad94f17-05f0-4c35-bae8-e1ad7befdcd2",
   "metadata": {},
   "source": [
    "Ensemble techniques offer several important benefits in machine learning, making them valuable tools for improving the performance and robustness of predictive models. Here are some of the key benefits of using ensemble techniques:\n",
    "\n",
    "Improved Predictive Accuracy:\n",
    "\n",
    "One of the primary advantages of ensemble techniques is that they often lead to higher predictive accuracy compared to individual base models. Combining the predictions of multiple models can capture a broader range of patterns in the data, reducing errors and improving overall performance.\n",
    "Better Generalization:\n",
    "\n",
    "Ensembles are effective at reducing overfitting, which occurs when a model performs well on the training data but poorly on unseen data. By aggregating predictions from diverse models, ensembles tend to generalize better to new, unseen data.\n",
    "Robustness to Noise and Outliers:\n",
    "\n",
    "Ensembles are more robust to noisy data and outliers than individual models. Outliers and noisy data points can have a disproportionate impact on single models but have less influence when combined with predictions from other models.\n",
    "Handling Complex Relationships:\n",
    "\n",
    "Ensemble methods can capture complex relationships in the data by leveraging diverse base models. This is particularly valuable when dealing with data that contains intricate, nonlinear patterns.\n",
    "Model Stability:\n",
    "\n",
    "Ensembles are often more stable and less sensitive to variations in the training data. Small changes in the training data or initial conditions are less likely to result in significant changes in ensemble predictions.\n",
    "Reduced Bias:\n",
    "\n",
    "Ensembles can help reduce model bias by aggregating predictions from different perspectives. This is particularly valuable when dealing with imbalanced datasets or biased training samples.\n",
    "State-of-the-Art Performance:\n",
    "\n",
    "Ensemble techniques are widely used in machine learning competitions and real-world applications to achieve state-of-the-art results. They have been instrumental in pushing the boundaries of what is achievable in predictive modeling.\n",
    "Model Interpretability:\n",
    "\n",
    "In some cases, ensembles can provide insights into feature importance or model contributions, making it possible to understand which aspects of the data are most influential in making predictions.\n",
    "Versatility:\n",
    "\n",
    "Ensemble techniques can be applied to various machine learning algorithms and models, making them versatile for different machine learning tasks, from classification to regression and more.\n",
    "Reduction of Variance and Bias:\n",
    "\n",
    "Ensembles can help balance the trade-off between bias and variance in predictive modeling. Bagging, for example, reduces variance, while boosting reduces bias.\n",
    "Parallelism and Scalability:\n",
    "\n",
    "Some ensemble methods, like bagging, are inherently parallelizable, making them suitable for distributed computing environments and speeding up training.\n",
    "Regularization:\n",
    "\n",
    "Ensembles can be seen as a form of regularization. By combining multiple models, they can reduce model complexity and prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df63359e-5818-4fb1-925e-f8edd7b13afe",
   "metadata": {},
   "source": [
    "Q6. Are ensemble techniques always better than individual models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b85801-e91f-4402-8481-77de24b925b8",
   "metadata": {},
   "source": [
    "Ensemble techniques are powerful tools in machine learning and often lead to improved predictive performance and robustness compared to individual models. However, whether ensemble techniques are always better than individual models depends on several factors, and there are scenarios where individual models may perform just as well or even better. Here are some considerations:\n",
    "\n",
    "Advantages of Ensemble Techniques:\n",
    "\n",
    "Improved Performance: Ensembles are particularly effective when the base models have complementary strengths and weaknesses. By combining diverse models, ensembles can capture a broader range of patterns in the data, leading to improved predictive accuracy.\n",
    "\n",
    "Robustness: Ensembles are more robust to noisy data, outliers, and model instability. The combination of multiple models helps mitigate the impact of individual model errors.\n",
    "\n",
    "Generalization: Ensembles tend to generalize better to new, unseen data, which is critical for real-world applications where model performance on out-of-sample data is essential.\n",
    "\n",
    "State-of-the-Art Results: In machine learning competitions and challenging tasks, ensembles are often used to achieve state-of-the-art results. They have a proven track record of success in many domains.\n",
    "\n",
    "Bias-Variance Trade-off: Ensemble techniques can help balance the bias-variance trade-off by reducing overfitting (variance) while improving model expressiveness (bias).\n",
    "\n",
    "When Individual Models May Perform Better:\n",
    "\n",
    "Simple Problems: For simple problems with a clear and easily discernible pattern, a single well-chosen model may perform as well as or better than an ensemble. Ensembles shine in complex and noisy datasets.\n",
    "\n",
    "Computational Resources: Ensembles can be computationally expensive, especially when dealing with large datasets or many base models. In cases where computational resources are limited, a single model may be preferred.\n",
    "\n",
    "Interpretability: Ensembles can be more challenging to interpret compared to individual models. For applications where interpretability and model transparency are essential, a single model might be preferred for ease of explanation.\n",
    "\n",
    "Training Data: If the training dataset is small and not representative of the target population, ensembles may not be as effective because they might amplify the noise present in the limited data.\n",
    "\n",
    "Overhead: Ensembling introduces additional complexity in model development, training, and deployment. If the problem can be adequately solved with a single model, avoiding the overhead of ensembling may be a practical choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7188035-da2d-4a1f-86a3-7aecab470de3",
   "metadata": {},
   "source": [
    "Q7. How is the confidence interval calculated using bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596969a3-b482-42ea-9de3-5aa391a9124a",
   "metadata": {},
   "source": [
    "The confidence interval calculated using bootstrap is a statistical technique that provides an estimate of the uncertainty or variability in a sample statistic by resampling the data with replacement. Bootstrap resampling is a powerful and flexible method for estimating the sampling distribution of a statistic and, consequently, for constructing confidence intervals.\n",
    "\n",
    "Here's a step-by-step explanation of how to calculate a confidence interval using bootstrap:\n",
    "\n",
    "Data Collection:\n",
    "\n",
    "Start with your original dataset, which contains observed data points.\n",
    "Resampling:\n",
    "\n",
    "Generate a large number (B) of bootstrap samples by randomly selecting data points from the original dataset with replacement. Each bootstrap sample should have the same size as the original dataset.\n",
    "Statistic Calculation:\n",
    "\n",
    "For each of the B bootstrap samples, calculate the statistic of interest. This statistic can be any measure, such as the mean, median, variance, standard deviation, or any other parameter you want to estimate.\n",
    "Sampling Distribution:\n",
    "\n",
    "You now have B resampled statistics, creating a sampling distribution of the statistic. This distribution approximates the variability of the statistic if you were to repeatedly sample from the population.\n",
    "Confidence Interval Construction:\n",
    "\n",
    "To construct a confidence interval, you need to determine the range of values within which the true population parameter (e.g., population mean) is likely to fall with a specified level of confidence (e.g., 95% confidence).\n",
    "Sort the B resampled statistics in ascending order.\n",
    "Calculate the lower and upper percentiles of the sorted statistics to create a confidence interval.\n",
    "For a 95% confidence interval, you would typically use the 2.5th percentile as the lower bound and the 97.5th percentile as the upper bound.\n",
    "The confidence interval will contain the range of values within which the population parameter is likely to lie with 95% confidence.\n",
    "Mathematically, the confidence interval can be expressed as:\n",
    "CI\n",
    "=\n",
    "(\n",
    "Percentile\n",
    "(\n",
    "Bootstrap Statistics\n",
    ",\n",
    "�\n",
    "2\n",
    ")\n",
    ",\n",
    "Percentile\n",
    "(\n",
    "Bootstrap Statistics\n",
    ",\n",
    "1\n",
    "−\n",
    "�\n",
    "2\n",
    ")\n",
    ")\n",
    "CI=(Percentile(Bootstrap Statistics, \n",
    "2\n",
    "α\n",
    "​\n",
    " ),Percentile(Bootstrap Statistics,1− \n",
    "2\n",
    "α\n",
    "​\n",
    " ))\n",
    "\n",
    "CI: Confidence Interval\n",
    "Bootstrap Statistics: The sorted resampled statistics\n",
    "�\n",
    "α: Significance level (e.g., 0.05 for a 95% confidence interval)\n",
    "The resulting confidence interval provides a range of values for the parameter of interest, along with the specified level of confidence, indicating the uncertainty associated with the estimate. It represents the range of values that you can be reasonably confident contains the true population parameter.\n",
    "\n",
    "Bootstrap-based confidence intervals are widely used in statistics and data analysis because they make fewer assumptions about the data distribution and can be applied to various types of statistics and parameters. They are particularly useful when the underlying population distribution is unknown or complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacc0028-9812-47bf-ab60-75960b1e6e22",
   "metadata": {},
   "source": [
    "Q8. How does bootstrap work and What are the steps involved in bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dbf315-3df2-4fe3-8260-239d2d29bb7b",
   "metadata": {},
   "source": [
    "Bootstrap is a resampling technique in statistics that is used to estimate the sampling distribution of a statistic by repeatedly resampling the data with replacement. It allows you to make inferences about population parameters, construct confidence intervals, and assess the variability of a statistic without making strong assumptions about the underlying data distribution. Here are the steps involved in the bootstrap procedure:\n",
    "\n",
    "Data Collection:\n",
    "\n",
    "Start with your original dataset, which contains observed data points. This dataset is often referred to as the \"population.\"\n",
    "Resampling:\n",
    "\n",
    "Generate a large number (B) of bootstrap samples by randomly selecting data points from the original dataset with replacement. Each bootstrap sample should have the same size as the original dataset.\n",
    "The \"with replacement\" aspect means that each data point in the original dataset can be selected multiple times in a single bootstrap sample, and some data points may be left out.\n",
    "Statistic Calculation:\n",
    "\n",
    "For each of the B bootstrap samples, calculate the statistic of interest. This statistic can be any measure you want to estimate, such as the mean, median, variance, standard deviation, or any other parameter.\n",
    "The goal is to create a distribution of this statistic by calculating it for each bootstrap sample.\n",
    "Sampling Distribution:\n",
    "\n",
    "You now have B resampled statistics, which form an empirical approximation of the sampling distribution of the statistic. This distribution reflects the variability you would observe if you were to repeatedly sample from the population.\n",
    "Confidence Interval Construction (optional):\n",
    "\n",
    "To construct a confidence interval for the statistic, you can calculate percentiles of the sampling distribution.\n",
    "For example, a common choice is to calculate the lower and upper percentiles (e.g., the 2.5th and 97.5th percentiles) to create a 95% confidence interval.\n",
    "The confidence interval provides a range of values within which the true population parameter is likely to fall with a specified level of confidence.\n",
    "Summary Statistics and Visualization:\n",
    "\n",
    "Often, you will compute summary statistics for the resampled statistics, such as the mean, standard error, and confidence intervals, to summarize the results.\n",
    "Visualization techniques, such as histograms or boxplots of the resampled statistics, can help you understand the distribution's shape and variability.\n",
    "Inference and Interpretation:\n",
    "\n",
    "You can use the bootstrap results to make statistical inferences and draw conclusions about the parameter of interest. For example, you might state that you are 95% confident that the true population mean falls within a certain interval.\n",
    "Interpret the results in the context of your specific problem or research question.\n",
    "Bootstrap is a powerful technique because it provides a way to estimate the distribution of a statistic without making strong assumptions about the data. It is particularly valuable when dealing with small sample sizes, non-normal data, or complex statistical problems. Bootstrap can be applied to various statistical analyses, hypothesis tests, and parameter estimation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6066a9bf-85c2-4ce1-8ddb-15e888d3dc7c",
   "metadata": {},
   "source": [
    "Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
    "sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
    "bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87f3091-69a6-4179-b130-dc8469e9f841",
   "metadata": {},
   "source": [
    "To estimate the 95% confidence interval for the population mean height of trees using the bootstrap method, you can follow these steps:\n",
    "\n",
    "Original Data:\n",
    "\n",
    "Start with the original data, which consists of the heights of the 50 sampled trees.\n",
    "Calculate the sample mean (\n",
    "�\n",
    "ˉ\n",
    "x\n",
    "ˉ\n",
    " ) and the sample standard deviation (\n",
    "�\n",
    "s) of these heights:\n",
    "Sample Mean (\n",
    "�\n",
    "ˉ\n",
    "x\n",
    "ˉ\n",
    " ) = 15 meters\n",
    "Sample Standard Deviation (\n",
    "�\n",
    "s) = 2 meters\n",
    "Bootstrap Resampling:\n",
    "\n",
    "Generate a large number (B) of bootstrap samples by randomly selecting 50 heights from the original data with replacement.\n",
    "For each bootstrap sample, calculate the sample mean (\n",
    "�\n",
    "ˉ\n",
    "bootstrap\n",
    "x\n",
    "ˉ\n",
    "  \n",
    "bootstrap\n",
    "​\n",
    " ) of the resampled heights.\n",
    "Sampling Distribution of the Mean:\n",
    "\n",
    "You now have a distribution of sample means (\n",
    "�\n",
    "ˉ\n",
    "bootstrap\n",
    "x\n",
    "ˉ\n",
    "  \n",
    "bootstrap\n",
    "​\n",
    " ) obtained from the bootstrap resamples. This approximates the sampling distribution of the mean height.\n",
    "Confidence Interval Calculation:\n",
    "\n",
    "To construct the 95% confidence interval, you need to find the 2.5th percentile and the 97.5th percentile of the distribution of sample means.\n",
    "The 2.5th percentile represents the lower bound of the confidence interval, and the 97.5th percentile represents the upper bound.\n",
    "Bootstrap Percentiles:\n",
    "\n",
    "Calculate the 2.5th and 97.5th percentiles of the distribution of sample means obtained from the bootstrap samples. These percentiles represent the lower and upper bounds of the 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9596510-753d-4bcc-aad6-bea2e72903ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval for Mean Height: (15.0, 15.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original data (sampled heights of 50 trees)\n",
    "original_data = np.array([15] * 50)\n",
    "\n",
    "# Number of bootstrap samples\n",
    "B = 10000\n",
    "\n",
    "# Bootstrap resampling\n",
    "bootstrap_means = []\n",
    "for _ in range(B):\n",
    "    bootstrap_sample = np.random.choice(original_data, size=50, replace=True)\n",
    "    bootstrap_mean = np.mean(bootstrap_sample)\n",
    "    bootstrap_means.append(bootstrap_mean)\n",
    "\n",
    "# Calculate percentiles for the confidence interval\n",
    "lower_percentile = np.percentile(bootstrap_means, 2.5)\n",
    "upper_percentile = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "# Confidence interval\n",
    "confidence_interval = (lower_percentile, upper_percentile)\n",
    "\n",
    "print(\"95% Confidence Interval for Mean Height:\", confidence_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e74b017-9941-4a6b-afa1-114a0304a595",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
